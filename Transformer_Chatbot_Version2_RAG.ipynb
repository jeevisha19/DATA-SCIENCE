{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNf3OIzN81ivOicRbUS5sdF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeevisha19/DATA-SCIENCE/blob/main/Transformer_Chatbot_Version2_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers faiss-cpu langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mB_7D011pYS9",
        "outputId": "6bdcbd80-36e4-4756-91e8-daf75ebf3e35"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.2.3)\n",
            "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cpu)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.1 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.2.6)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.5)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.6.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (9.1.2)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.13.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.3.1)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.1->langchain) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.1)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (0.25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2026.1.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (0.16.0)\n",
            "Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.13.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\n",
        "    \"Natural Language Processing is a branch of artificial intelligence that focuses on the interaction between computers and human language.\",\n",
        "    \"Transformers are deep learning models based on self-attention mechanisms and are widely used in NLP tasks.\",\n",
        "    \"Machine learning allows systems to learn patterns from data without explicit programming.\",\n",
        "    \"Deep learning uses neural networks with multiple layers to learn complex representations.\"\n",
        "]"
      ],
      "metadata": {
        "id": "XodGIc9npcu8"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "doc_embeddings = embedder.encode(documents)"
      ],
      "metadata": {
        "id": "7YQUJn_5pfxr"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "dimension = doc_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(np.array(doc_embeddings))"
      ],
      "metadata": {
        "id": "hVvYbonwptWm"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_docs(query, top_k=2):\n",
        "    query_embedding = embedder.encode([query])\n",
        "    distances, indices = index.search(np.array(query_embedding), top_k)\n",
        "    return [documents[i] for i in indices[0]]"
      ],
      "metadata": {
        "id": "MDqmOsdDpynK"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_chat(query):\n",
        "    # 1. Retrieve relevant documents\n",
        "    retrieved_docs = retrieve_docs(query, top_k=2)\n",
        "\n",
        "    # 2. Build short context\n",
        "    context = \" \".join(retrieved_docs)\n",
        "\n",
        "    # 3. Augment input\n",
        "    augmented_input = context + \"\\nQuestion: \" + query + \"\\nAnswer:\"\n",
        "\n",
        "    # 4. Tokenize\n",
        "    input_ids = tokenizer.encode(\n",
        "        augmented_input,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "    # 5. Generate controlled response\n",
        "    output_ids = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=80,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "        temperature=0.7,\n",
        "        repetition_penalty=1.2,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # 6. Decode and clean output\n",
        "    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    response = response.split(\"Answer:\")[-1].strip()\n",
        "\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "9e5tknU6p_Jd"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    query = input(\"You: \")\n",
        "    if query.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
        "        print(\"RAG Bot: Goodbye!\")\n",
        "        break\n",
        "    print(\"RAG Bot:\", rag_chat(query))\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlxbUGgSqIG0",
        "outputId": "000eb6d7-13d6-4b56-973c-e1d818aa914d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: bye\n",
            "RAG Bot: Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation for RAG\n"
      ],
      "metadata": {
        "id": "rpgGtg4MvPnc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "EVALUATE RETRIEVAL QUALITY (RECALL@K)"
      ],
      "metadata": {
        "id": "ISWTWEXS71OF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = [\n",
        "    {\n",
        "        \"question\": \"What is natural language processing?\",\n",
        "        \"ground_truth\": \"Natural language processing is a branch of artificial intelligence that focuses on the interaction between computers and human language.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What are transformers?\",\n",
        "        \"ground_truth\": \"Transformers are deep learning models based on self-attention mechanisms used in NLP.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is deep learning?\",\n",
        "        \"ground_truth\": \"Deep learning uses neural networks with multiple layers to learn complex representations.\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "Ebip4lmI7Z-r"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieval_recall_at_k(test_data, k=2):\n",
        "    correct = 0\n",
        "\n",
        "    for item in test_data:\n",
        "        retrieved_docs = retrieve_docs(item[\"question\"], top_k=k)\n",
        "        if any(item[\"ground_truth\"].lower() in doc.lower() for doc in retrieved_docs):\n",
        "            correct += 1\n",
        "\n",
        "    return correct / len(test_data)\n"
      ],
      "metadata": {
        "id": "Z9ObW1k17csX"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recall_k = retrieval_recall_at_k(test_data, k=2)\n",
        "print(\"Retrieval Recall@2:\", recall_k)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1m1cFDO7ouy",
        "outputId": "4eb8f523-3cf4-4094-a776-9d6737653deb"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieval Recall@2: 0.6666666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EVALUATE ANSWER QUALITY\n"
      ],
      "metadata": {
        "id": "ezlr4EBP74SX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def semantic_similarity_score(test_data):\n",
        "    scores = []\n",
        "\n",
        "    for item in test_data:\n",
        "        response = rag_chat(item[\"question\"])\n",
        "\n",
        "        emb_response = embedder.encode([response])\n",
        "        emb_truth = embedder.encode([item[\"ground_truth\"]])\n",
        "\n",
        "        sim = cosine_similarity(emb_response, emb_truth)[0][0]\n",
        "        scores.append(sim)\n",
        "\n",
        "    return np.mean(scores)\n"
      ],
      "metadata": {
        "id": "je1M9yjL78Lj"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_score = semantic_similarity_score(test_data)\n",
        "print(\"Average Semantic Similarity:\", similarity_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oTOwY4z8HZz",
        "outputId": "2aa1c69c-bb4b-4fe1-efae-71a3276d8046"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Semantic Similarity: 0.61590594\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparision with NO RAG version"
      ],
      "metadata": {
        "id": "S4bnKRwphZhq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a baseline function:"
      ],
      "metadata": {
        "id": "RrCRrJKVhk14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def no_rag_chat(query):\n",
        "    input_ids = tokenizer.encode(\n",
        "        query + tokenizer.eos_token,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    output_ids = model.generate(\n",
        "        input_ids,\n",
        "        max_length=300,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "aN1M-9U3hlr9"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare Similarity:"
      ],
      "metadata": {
        "id": "cMMQ5kWJhrW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def compare_rag_vs_no_rag(test_data):\n",
        "    rag_scores = []\n",
        "    no_rag_scores = []\n",
        "\n",
        "    for item in test_data:\n",
        "        rag_resp = rag_chat(item[\"question\"])\n",
        "        no_rag_resp = no_rag_chat(item[\"question\"])\n",
        "\n",
        "        emb_truth = embedder.encode([item[\"ground_truth\"]])\n",
        "\n",
        "        rag_emb = embedder.encode([rag_resp])\n",
        "        no_rag_emb = embedder.encode([no_rag_resp])\n",
        "\n",
        "        rag_scores.append(cosine_similarity(rag_emb, emb_truth)[0][0])\n",
        "        no_rag_scores.append(cosine_similarity(no_rag_emb, emb_truth)[0][0])\n",
        "\n",
        "    return np.mean(no_rag_scores), np.mean(rag_scores)"
      ],
      "metadata": {
        "id": "ZzeiYe0HhuOV"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_rag, rag = compare_rag_vs_no_rag(test_data)\n",
        "\n",
        "print(\"No-RAG Similarity:\", no_rag)\n",
        "print(\"RAG Similarity:\", rag)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWyKkkKVjWzl",
        "outputId": "00f23a22-7604-4a3b-d6c1-48bd997435cd"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No-RAG Similarity: 0.27347732\n",
            "RAG Similarity: 0.55999523\n"
          ]
        }
      ]
    }
  ]
}